# ============================================
# Voice AI POC - Environment Variables
# ============================================

# Modo de execução: 'local' (microfone/speaker) ou 'telnyx' (telefonia)
MODE=local

# ============================================
# TELNYX (Telefonia) - Só necessário se MODE=telnyx
# ============================================
TELNYX_API_KEY=your_telnyx_api_key
TELNYX_CONNECTION_ID=your_connection_id
TELNYX_PHONE_NUMBER=+5511999999999

# ============================================
# OPENAI (LLM)
# ============================================
OPENAI_API_KEY=your_openai_api_key

# Modelo do LLM - Opções (ordenadas por latência):
# - gpt-3.5-turbo: ~300-500ms TTFT, qualidade aceitável
# - gpt-4o-mini: ~400-900ms TTFT, boa qualidade (RECOMENDADO)
# - gpt-4o: ~800-1200ms TTFT, excelente qualidade
LLM_MODEL=gpt-4o-mini

# ============================================
# STT (Speech-to-Text)
# ============================================
# Provider: 'elevenlabs' (Scribe streaming ~100-300ms) ou 'openai' (Whisper batch ~800-2000ms)
STT_PROVIDER=elevenlabs

# ============================================
# ELEVENLABS (TTS + STT Scribe)
# ============================================
# TTS: ~75-100ms TTFB, excelente qualidade de voz
# STT: Scribe streaming ~100-300ms
ELEVENLABS_API_KEY=your_elevenlabs_api_key
ELEVENLABS_VOICE_ID=pFZP5JQG7iQjIQuC4Bku

# ============================================
# OTIMIZAÇÕES DE LATÊNCIA
# ============================================

# Usar prompt reduzido (~70% menos tokens, menor latência no 1º turno)
# true = prompt slim, false = prompt completo
SLIM_PROMPT=false

# Pré-carregar fillers de áudio no startup (latência zero durante chamada)
FILLERS_PRELOAD_ON_STARTUP=true

# ============================================
# THINKING ENGINE (experimental)
# ============================================
# Quando habilitado, faz uma chamada LLM adicional por turno (durante playback)
# para processar "pensamentos internos" do agente (metacognição)
# 
# Benefícios: Melhor coerência e raciocínio estratégico na conversa
# Trade-off: ~2x tokens consumidos por turno
#
# Recomendação: Manter false para produção até validar valor vs custo
ENABLE_THINKING_ENGINE=false

# ============================================
# SERVER
# ============================================
PORT=3000
HOST=0.0.0.0
WEBHOOK_URL=https://your-domain.com/webhook

# ============================================
# DEBUG
# ============================================
LOG_LEVEL=info

# ============================================
# MODO LOCAL (opcional - para personalizar conversa)
# ============================================
PROSPECT_NAME=João
PROSPECT_COMPANY=Empresa Teste

# ============================================
# BENCHMARKS DE REFERÊNCIA (Cloud(x) 2025)
# ============================================
# 
# | Stack                          | TTFA (1º turno) | TTFA (subseq.) |
# |--------------------------------|-----------------|----------------|
# | GPT-4o-mini + ElevenLabs       | ~1.2-1.5s       | ~0.7-1.0s      |
# | GPT-3.5-turbo + ElevenLabs     | ~0.9-1.2s       | ~0.5-0.8s      |
#
# Target de latência aceitável: < 1500ms (ITU-T G.114: < 400ms é "natural")
